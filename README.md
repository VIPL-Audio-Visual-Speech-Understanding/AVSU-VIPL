# Works Collection of VIPL-AVSU-Group

This is a collection of works from the Audio-Visual Speech Understanding Group at VIPL.

Our group website is [here](http://vipl.ict.ac.cn/en/team.php?id=9).

## Recent News: 
[**2025-05**]: 1 paper is accepted by IEEE FG 2025! Congratulations to Songtao!

[**2024-12**]: Start of Challenge [MAVSR-2025 @ IEEE FG 2025](https://codalab.lisn.upsaclay.fr/competitions/21609)! Welcome to the competition! 

[**2024-06**]: Championship in the open track of the AVSE Challenge @ InterSpeech 2024! Congratulations to Fei-Xiang! 

[**2024-02**]: 1 paper is accepted by CVPR 2024! Congratulations to Yuanhang!

[**2023-08**]: 3 papers are accepted by BMVC 2023! Congratulations to Bing-Quan, Song-Tao and Fei-Xiang!

[**2022-06**]: Championship again of the AVA Active Speaker Challenge @ CVPR 2022! More details can be found [here](https://research.google.com/ava/challenge.html). Congratulations to Yuanhang and Susan!

[**2022-03**]: 1 paper is accepted by ICPR 2022! Congratulations to Dalu!

[**2021-07**]: 1 paper is accepted by ICME Workshop 2021! Congratulations to Dalu!

[**2021-07**]: 1 paper is accepted by ACM MM 2021! Congratulations to Yuanhang and Susan!

[**2021-06**]: Champion of the AVA Active Speaker Challenge @ CVPR 2021! More details can be found [here](https://research.google.com/ava/challenge.html). Congratulations to Yuanhang and Susan!

## Datasets

### CAS-VSR-S101: A dataset for sentence-level audio visual speech analysis, CVPR 2024
  **CAS-VSR-S101** is a Mandarin audio visual speech analysis dataset, involving almost all common Chinese characters and numbers of speakers speaking in diversed visual settings.
  * Dataset Link: [https://github.com/VIPL-Audio-Visual-Speech-Understanding/CAS-VSR-S101](https://github.com/VIPL-Audio-Visual-Speech-Understanding/CAS-VSR-S101)
  * Paper Link: [CAS-VSR-S101 paper](https://openaccess.thecvf.com/content/CVPR2024/papers/Zhang_ES3_Evolving_Self-Supervised_Learning_of_Robust_Audio-Visual_Speech_Representations_CVPR_2024_paper.pdf)
  > *Note:* You can download the agreement file [here]([https://github.com/jinchiniao/CAS-VSR-S68/blob/main/CAS-VSR-S68-Release%20Agreement-v3.pdf](https://github.com/VIPL-Audio-Visual-Speech-Understanding/CAS-VSR-S101)). Please read the agreement carefully, and complete it appropriately. Note that the agreement should be signed by **a full-time staff member** (that is, students are not acceptable). Then, please scan the signed agreement and send it to lipreading@vipl.ict.ac.cn. When we receive your reply, we will provide the download link to you as soon as possible. 

### LRW-1000: A naturally-distributed large-scale benchmark for lip reading in the wild, FG 2019
  The largest Mandarin word-level audio-visual speech recognition dataset (2022), also called **CAS-VSR-W1k**.
  * Dataset Link：[http://vipl.ict.ac.cn/en/view_database.php?id=13](http://vipl.ict.ac.cn/en/view_database.php?id=13)  
  * Application Agreement: [link1](https://vipl.ict.ac.cn/uploadfile/upload/2019120612315190.pdf) or [link2](https://github.com/VIPL-Audio-Visual-Speech-Understanding/AVSU-VIPL/blob/master/LRW-1000-Release%20Agreement.pdf) 
  * Codes: DenseNet3D [@fengdalu](https://github.com/VIPL-Audio-Visual-Speech-Understanding/Lipreading-DenseNet3D) [@NirHeaven](https://github.com/NirHeaven/D3D)
  * SOTA Accuracies: [https://paperswithcode.com/sota/lipreading-on-lrw-1000](https://paperswithcode.com/sota/lipreading-on-lrw-1000)
  > *Note:* If you cannot open the website for the dataset, you can go to the paper page for details about the data, and then download the agreement file [here](https://github.com/VIPL-Audio-Visual-Speech-Understanding/AVSU-VIPL/blob/master/LRW-1000-Release%20Agreement.pdf) in this repository if you plan to use this dataset for your research. Please read the agreement carefully, and complete it appropriately. Note that the agreement should be signed by **a full-time staff member** (that is, students are not acceptable). Then, please scan the signed agreement and send it to lipreading@vipl.ict.ac.cn. When we receive your reply, we will provide the download link to you as soon as possible. 

### CAS-VSR-S68: A dataset for lip reading with unseen speakers, BMVC 2023
  **CAS-VSR-S68** is a lip reading dataset designed for evaluation of the extreme setting where the speech content is highly diverse, involving almost all common Chinese characters while the number of speakers is limited.
  * Dataset Link: [https://github.com/jinchiniao/CAS-VSR-S68](https://github.com/jinchiniao/CAS-VSR-S68)
  * Paper Link: [https://arxiv.org/abs/2310.05058](https://arxiv.org/abs/2310.05058)
  > *Note:* You can download the agreement file [here](https://github.com/jinchiniao/CAS-VSR-S68/blob/main/CAS-VSR-S68-Release%20Agreement-v3.pdf). Please read the agreement carefully, and complete it appropriately. Note that the agreement should be signed by **a full-time staff member** (that is, students are not acceptable). Then, please scan the signed agreement and send it to lipreading@vipl.ict.ac.cn. When we receive your reply, we will provide the download link to you as soon as possible. 
## Challenges
### 2025-The 2nd Mandarin Audio-Visual Speech Recognition Challenge (MAVSR) @ IEEE FG
  Welcome to the competition!
  * Introduction @IEEE FG Website: [https://fg2025.ieee-biometrics.org/participate/competitions/](https://fg2025.ieee-biometrics.org/participate/competitions/ )
  * Homepage: [here](https://codalab.lisn.upsaclay.fr/competitions/21609)
  * Date: 2024/12 - 2025/05

### 2022 世界机器人大赛-共融机器人挑战赛-语音识别技术赛
  * Homepage: [here](http://www.worldrobotconference.com/cn/about/139.html)
  * Date: 2022/06-2022/12
  * 欢迎报名！
  
### 2019-The 1st Mandarin Audio-Visual Speech Recognition Challenge (MAVSR) @ ACM ICMI
  This challenge aims at exploring the complementarity between visual and acoustic information in real-world speech recognition systems.
  * Introduction @ICMI Website: [https://icmi.acm.org/2019/index.php?id=challenges#speech](https://icmi.acm.org/2019/index.php?id=challenges#speech)
  * Homepage: [here](https://vipl.ict.ac.cn/zygx/lthjs/202206/t20220605_37899.html)
  * Date: 2019/04 - 2019/08

## Publications
* Yuanhang Zhang, Shuang Yang, Shiguang Shan, Xilin Chen, "ES3: Evolving Self-Supervised Learning of Robust Audio-Visual Speech Representations", *CVPR* 2024.

* Dalu Feng, Shuang Yang, Shiguang Shan, Xilin Chen, "Audio-guided self-supervised learning for disentangled visual speech representations", *Frontiers of Computer Science*, 2024.

* Feixiang Wang, Shuang Yang, Shiguang Shan, Xilin Chen, "Cooperative Dual Attention for Audio-Visual Speech Enhancement with Facial Cues",  *BMVC* 2023. 

* Bingquan Xia, Shuang Yang, Shiguang Shan, Xilin Chen. "UniLip: Learning Visual-Textual Mapping with Uni-Modal Data for Lip Reading".  *BMVC* 2023.

* Songtao Luo, Shuang Yang, Shiguang Shan, Xilin Chen. "Learning Separable Hidden Unit Contributions for Speaker-Adaptive Lip-Reading", *BMVC* 2023. [[PDF]](https://arxiv.org/pdf/2310.05058.pdf) | [[Dataset]](https://github.com/jinchiniao/CAS-VSR-S68) | [[code]](https://github.com/jinchiniao/LSHUC)

* Yuanhang Zhang, Susan Liang, Shuang Yang, Shiguang Shan, "Unicon+: ICTCAS-UCAS-TAL Submission to the AVA-ActiveSpeaker Task at ActivityNet Challenge 2022", The ActivityNet Large-Scale Activity Recognition Challenge at CVPR 2022 (**1st Place**).

* Dalu Feng, Shuang Yang, Shiguang Shan, Xilin Chen, "Audio-Driven Deformation Flow for Effective Lip Reading", *ICPR* 2022

* Yuanhang Zhang, Susan Liang, Shuang Yang, Xiao Liu, Zhongqin Wu, Shiguang Shan, "ICTCAS-UCAS-TAL Submission to the AVA-ActiveSpeaker Task at ActivityNet Challenge 2021", The ActivityNet Large-Scale Activity Recognition Challenge at CVPR 2021 (**1st Place**). [[PDF]](http://static.googleusercontent.com/media/research.google.com/zh-CN//ava/2021/S1_ICTCAS-UCAS-TAL.pdf)

* Yuanhang Zhang, Susan Liang, Shuang Yang, Xiao Liu, Zhongqin Wu, Shiguang Shan, Xilin Chen, "UniCon: Unified Context Network for Robust Active Speaker Detection", *ACM MM* 2021.(**Oral**). [[Website]](https://unicon-asd.github.io/) | [[PDF]](https://arxiv.org/pdf/2108.02607.pdf)

* Dalu Feng, Shuang Yang, Shiguang Shan, Xilin Chen, "Learn an Effective Lip Reading Model without Pains", ICME Workshop 2021  
   [[PDF]](https://arxiv.org/abs/2011.07557) |  [[code]](https://github.com/VIPL-Audio-Visual-Speech-Understanding/learn-an-effective-lip-reading-model-without-pains)

* Mingshuang Luo, Shuang Yang, Shiguang Shan, Xilin Chen, "Synchronous Bidirectional Learning for Multilingual Lip Reading", *BMVC* 2020  
    [[PDF]](https://vipl.ict.ac.cn/uploadfile/upload/2020093011033041.pdf)  | [[code]](https://github.com/luomingshuang/SBL_For_Multilingual_Lip_Reading)

* Jingyun Xiao, Shuang Yang, Yuanhang Zhang, Shiguang Shan, Xilin Chen, "Deformation Flow Based Two-Stream Network for Lip Reading", *FG* 2020  
    [[PDF]](https://vipl.ict.ac.cn/uploadfile/upload/2020071411144684.pdf) | [[code]](https://github.com/jingyunx/Deformation-Flow-Based-Two-stream-Network)

* Xing Zhao, Shuang Yang, Shiguang Shan, Xilin Chen, "Mutual Information Maximization for Effective Lipreading", *FG* 2020  
    [[PDF]](https://vipl.ict.ac.cn/uploadfile/upload/2020071411172971.pdf) | [[code]](https://github.com/xing96/MIM-lipreading)
  
* Yuanhang Zhang, Shuang Yang, Jingyun Xiao, Shiguang Shan, Xilin Chen, "Can We Read Speech Beyond the Lips? Rethinking RoI Selection for Deep Visual Speech Recognition", *FG* 2020 **(oral)**  
    [[PDF]](https://vipl.ict.ac.cn/uploadfile/upload/2020071411181845.pdf) | [[code]](https://github.com/VIPL-Audio-Visual-Speech-Understanding/deep-face-speechreading)
  
* Mingshuang Luo, Shuang Yang, Shiguang Shan, Xilin Chen, "Pseudo-Convolutional Policy Gradient for Sequence-to-Sequence Lip-Reading", *FG* 2020  
    [[PDF]](https://vipl.ict.ac.cn/uploadfile/upload/2020071411152795.pdf)
  
* Yuanhang Zhang, Jingyun Xiao, Shuang Yang, Shiguang Shan, "Multi-Task Learning for Audio-Visual Active Speaker Detection", *CVPR ActivityNet Challenge* 2019  
    [[PDF]](https://static.googleusercontent.com/media/research.google.com/zh-CN//ava/2019/Multi_Task_Learning_for_Audio_Visual_Active_Speaker_Detection.pdf)

* Yang Shuang, Yuanhang Zhang, Dalu Feng, Mingmin Yang, Chenhao Wang, Jingyun Xiao, Keyu Long, Shiguang Shan, and Xilin Chen. "LRW-1000: A naturally-distributed large-scale benchmark for lip reading in the wild." *FG* 2019 [[PDF]](https://arxiv.org/abs/1810.06990) | [[Dataset]](http://vipl.ict.ac.cn/en/view_database.php?id=13) | [Code@fengdalu](https://github.com/VIPL-Audio-Visual-Speech-Understanding/Lipreading-DenseNet3D) [Code@NirHeaven](https://github.com/NirHeaven/D3D)  

    
