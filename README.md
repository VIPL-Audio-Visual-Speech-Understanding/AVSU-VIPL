# Collection of works from VIPL-AVSU

This is a collection of works from the Audio-Visual Speech Understanding Group at VIPL.

Our group website is [here](http://vipl.ict.ac.cn/en/team.php?id=9).

## Recent News: 
[**2022-06**]: Retain the **championship** at AVA Active Speaker Challenge @ CVPR 2022: We achieved an mAP of **94.5%** on AVA-ActiveSpeaker Dataset and obtained the **1st place prize** in the Active Speaker Detection Track. More details can be found [here](https://research.google.com/ava/challenge.html). Congratulations to Yuanhang and Susan!

[**2022-06**]: One paper is accepted by ICPR 2022! Congratulations to Dalu!

[**2021-07**]: One paper is accepted by ICME Workshop 2021! Congratulations to Dalu!

[**2021-07**]: One paper is accepted by ACM MM 2021! Congratulations to Yuanhang and Susan!

[**2021-06**]: AVA Challenge as part of ActivityNet @CVPR-2021: We achieved an mAP of **93.4%** on AVA-ActiveSpeaker Dataset and obtained the **1st place prize** in the Active Speaker Detection Track. More details can be found [here](https://research.google.com/ava/challenge.html). Congratulations to Yuanhang and Susan!

## Datasets

### LRW-1000: A naturally-distributed large-scale benchmark for lip reading in the wild, FG 2019
  The largest Mandarin word-level audio-visual speech recognition dataset (2022), also called **CAS-VSR-W1k**.
  * Dataset Linkï¼š[http://vipl.ict.ac.cn/en/view_database.php?id=13](http://vipl.ict.ac.cn/en/view_database.php?id=13)  
  * Application Agreement: [https://vipl.ict.ac.cn/uploadfile/upload/2019120612315190.pdf](https://vipl.ict.ac.cn/uploadfile/upload/2019120612315190.pdf) or [[https://github.com/VIPL-Audio-Visual-Speech-Understanding/AVSU-VIPL](https://github.com/VIPL-Audio-Visual-Speech-Understanding/AVSU-VIPL/blob/master/LRW-1000-Release%20Agreement.pdf)]([https://github.com/VIPL-Audio-Visual-Speech-Understanding/AVSU-VIPL](https://github.com/VIPL-Audio-Visual-Speech-Understanding/AVSU-VIPL/blob/master/LRW-1000-Release%20Agreement.pdf)) 
  * Codes: DenseNet3D [@fengdalu](https://github.com/VIPL-Audio-Visual-Speech-Understanding/Lipreading-DenseNet3D) [@NirHeaven](https://github.com/NirHeaven/D3D)
  * SOTA Accuracies: [https://paperswithcode.com/sota/lipreading-on-lrw-1000](https://paperswithcode.com/sota/lipreading-on-lrw-1000)
  > *Note:* If you cannot open the website for the dataset, you can go to the paper page for details about the data, and then download the agreement file [here](https://github.com/VIPL-Audio-Visual-Speech-Understanding/AVSU-VIPL/blob/master/LRW-1000-Release%20Agreement.pdf) in this repository if you plan to use this dataset for your research. Please read the agreement carefully, and complete it appropriately. Note that the agreement should be signed by **a full-time staff member** (that is, students are not acceptable). Then, please scan the signed agreement and send it to lipreading@vipl.ict.ac.cn. When we receive your reply, we will provide the download link to you as soon as possible. 

## Challenges

### The 1st Mandarin Audio-Visual Speech Recognition Challenge (MAVSR)
  This challenge aims at exploring the complementarity between visual and acoustic information in real-world speech recognition systems.
  * Introduction @ICMI Website: [https://icmi.acm.org/2019/index.php?id=challenges#speech](https://icmi.acm.org/2019/index.php?id=challenges#speech)
  * Homepage: [http://vipl.ict.ac.cn/homepage/mavsr/index.html](http://vipl.ict.ac.cn/homepage/mavsr/index.html)
  
## Publications

* Yuanhang Zhang, Susan Liang, Shuang Yang, Shiguang Shan, "Unicon+: ICTCAS-UCAS-TAL Submission to the AVA-ActiveSpeaker Task at ActivityNet Challenge 2022", The ActivityNet Large-Scale Activity Recognition Challenge at CVPR 2022 (**1st Place**).

* Dalu Feng, Shuang Yang, Shiguang Shan, Xilin Chen, "Audio-Driven Deformation Flow for Effective Lip Reading", *ICPR* 2022

* Yuanhang Zhang, Susan Liang, Shuang Yang, Xiao Liu, Zhongqin Wu, Shiguang Shan, "ICTCAS-UCAS-TAL Submission to the AVA-ActiveSpeaker Task at ActivityNet Challenge 2021", The ActivityNet Large-Scale Activity Recognition Challenge at CVPR 2021 (**1st Place**). [[PDF]](http://static.googleusercontent.com/media/research.google.com/zh-CN//ava/2021/S1_ICTCAS-UCAS-TAL.pdf)

* Yuanhang Zhang, Susan Liang, Shuang Yang, Xiao Liu, Zhongqin Wu, Shiguang Shan, Xilin Chen, "UniCon: Unified Context Network for Robust Active Speaker Detection", *ACM MM* 2021.(**Oral**). [[Website]](https://unicon-asd.github.io/) | [[PDF]](https://arxiv.org/pdf/2108.02607.pdf)

* Dalu Feng, Shuang Yang, Shiguang Shan, Xilin Chen, "Learn an Effective Lip Reading Model without Pains", ICME Workshop 2021  
   [[PDF]](https://arxiv.org/abs/2011.07557) |  [[code]](https://github.com/VIPL-Audio-Visual-Speech-Understanding/learn-an-effective-lip-reading-model-without-pains)

* Mingshuang Luo, Shuang Yang, Shiguang Shan, Xilin Chen, "Synchronous Bidirectional Learning for Multilingual Lip Reading", *BMVC* 2020  
    [[PDF]](https://vipl.ict.ac.cn/uploadfile/upload/2020093011033041.pdf)  | [[code]](https://github.com/luomingshuang/SBL_For_Multilingual_Lip_Reading)

* Jingyun Xiao, Shuang Yang, Yuanhang Zhang, Shiguang Shan, Xilin Chen, "Deformation Flow Based Two-Stream Network for Lip Reading", *FG* 2020  
    [[PDF]](https://vipl.ict.ac.cn/uploadfile/upload/2020071411144684.pdf) | [[code]](https://github.com/jingyunx/Deformation-Flow-Based-Two-stream-Network)

* Xing Zhao, Shuang Yang, Shiguang Shan, Xilin Chen, "Mutual Information Maximization for Effective Lipreading", *FG* 2020  
    [[PDF]](https://vipl.ict.ac.cn/uploadfile/upload/2020071411172971.pdf) | [[code]](https://github.com/xing96/MIM-lipreading)
  
* Yuanhang Zhang, Shuang Yang, Jingyun Xiao, Shiguang Shan, Xilin Chen, "Can We Read Speech Beyond the Lips? Rethinking RoI Selection for Deep Visual Speech Recognition", *FG* 2020 **(oral)**  
    [[PDF]](https://vipl.ict.ac.cn/uploadfile/upload/2020071411181845.pdf) | [[code]](https://github.com/VIPL-Audio-Visual-Speech-Understanding/deep-face-speechreading)
  
* Mingshuang Luo, Shuang Yang, Shiguang Shan, Xilin Chen, "Pseudo-Convolutional Policy Gradient for Sequence-to-Sequence Lip-Reading", *FG* 2020  
    [[PDF]](https://vipl.ict.ac.cn/uploadfile/upload/2020071411152795.pdf)
  
* Yuanhang Zhang, Jingyun Xiao, Shuang Yang, Shiguang Shan, "Multi-Task Learning for Audio-Visual Active Speaker Detection", *CVPR ActivityNet Challenge* 2019  
    [[PDF]](https://static.googleusercontent.com/media/research.google.com/zh-CN//ava/2019/Multi_Task_Learning_for_Audio_Visual_Active_Speaker_Detection.pdf)

* Yang Shuang, Yuanhang Zhang, Dalu Feng, Mingmin Yang, Chenhao Wang, Jingyun Xiao, Keyu Long, Shiguang Shan, and Xilin Chen. "LRW-1000: A naturally-distributed large-scale benchmark for lip reading in the wild." *FG* 2019 [[PDF]](https://arxiv.org/abs/1810.06990) | [[Dataset]](http://vipl.ict.ac.cn/en/view_database.php?id=13) | [Code@fengdalu](https://github.com/VIPL-Audio-Visual-Speech-Understanding/Lipreading-DenseNet3D) [Code@NirHeaven](https://github.com/NirHeaven/D3D)  

    
