# Collection of the works from VIPL-AVSU

This is a collection of the works from Audio-Visual Speech Understading Group in VIPL.

Our group website is [here](http://vipl.ict.ac.cn/en/team.php?id=9).

## Recent News: 

[**2021-07**]: One paper is accepted by ACM MM 2021! Congratualtions to Yuanhang and Susan!

[**2021-06**]: AVA Challenge as part of ActivityNet @CVPR-2021: We achieved the mAP of **93.4%** on AVA Active Speaker Dataset and obtained the **1st place prize** in Active Speaker Track. More details could be found [here](https://research.google.com/ava/challenge.html). Congratualtions to Yuanhang and Susan!

## Datasets
### [AVSR DATASET]: (CAS-VSR-W1k) LRW-1000: A naturally-distributed large-scale benchmark for lip reading in the wild, FG 2019
  * dataset：[http://vipl.ict.ac.cn/en/view_database.php?id=13](http://vipl.ict.ac.cn/en/view_database.php?id=13)  
  * pdf: [https://vipl.ict.ac.cn/uploadfile/upload/2019120612315190.pdf](https://vipl.ict.ac.cn/uploadfile/upload/2019120612315190.pdf)  
  * \*code: [https://github.com/NirHeaven/D3D](https://github.com/NirHeaven/D3D)   | [https://github.com/Fengdalu/Lipreading-DenseNet3D](https://github.com/Fengdalu/Lipreading-DenseNet3D)  
  * SOTA Acc: [https://paperswithcode.com/sota/lipreading-on-lrw-1000](https://paperswithcode.com/sota/lipreading-on-lrw-1000)
  * *Note:* If you could not open the website of the dataset, you can go to the paper page for details about the data and further download the agreement file here in this repository if you plans to use this dataset for your research. Please read the agreement carefully, and complete it appropriately. Note that the agreement should be signed by a full-time staff member (that is, student is not acceptable). Then, please scan the signed agreement and send it to lipreading@vipl.ict.ac.cn. When we receive your reply, we would provide the download link to you as soon as possible. 

## Challenges
### [AVSR Challenge]: The Mandarin Audio-Visual Speech Recognition Challenge (MAVSR)
  * 2019: @ACM ICMI, [http://vipl.ict.ac.cn/homepage/mavsr/index.html](http://vipl.ict.ac.cn/homepage/mavsr/index.html)

#### Publications:

* Yuanhang Zhang, Susan Liang, Shuang Yang, Xiao Liu, Zhongqin Wu, Shiguang Shan, "ICTCAS-UCAS-TAL Submission to the AVA-ActiveSpeaker Task at ActivityNet Challenge 2021",The ActivityNet Large-Scale Activity Recognition Challenge @CVPR-2021.(**The 1st Place**). [[PDF]](http://static.googleusercontent.com/media/research.google.com/zh-CN//ava/2021/S1_ICTCAS-UCAS-TAL.pdf)

* Yuanhang Zhang, Susan Liang, Shuang Yang, Xiao Liu, Zhongqin Wu, Shiguang Shan, Xilin Chen, "UniCon: Unified Context Network for Robust Active Speaker Detection", *ACM MM* 2021.(**Oral**). [[Info]](https://unicon-asd.github.io/) | [[PDF]](https://arxiv.org/pdf/2108.02607.pdf)

* Dalu Feng, Shuang Yang, Shiguang Shan, Xilin Chen, “Learn an Effective Lip Reading Model without Pains”, ICME Workshop 2021  
   [[PDF]](https://arxiv.org/abs/2011.07557) |  [[code]](https://github.com/Fengdalu/learn-an-effective-lip-reading-model-without-pains)

* Mingshuang Luo, Shuang Yang, Shiguang Shan, Xilin Chen, “Synchronous Bidirectional Learning for Multilingual Lip Reading”, *BMVC* 2020  
    [[PDF]](https://vipl.ict.ac.cn/uploadfile/upload/2020093011033041.pdf) 

* Jingyun Xiao, Shuang Yang, Yuanhang Zhang, Shiguang Shan, Xilin Chen, “Deformation Flow Based Two-Stream Network for Lip Reading”, *FG* 2020  
    [[PDF]](https://vipl.ict.ac.cn/uploadfile/upload/2020071411144684.pdf) | [[code]](https://github.com/jingyunx/Deformation-Flow-Based-Two-stream-Network)

* Xing Zhao, Shuang Yang, Shiguang Shan, Xilin Chen, “Mutual Information Maximization for Effective Lipreading”, *FG* 2020  
    [[PDF]](https://vipl.ict.ac.cn/uploadfile/upload/2020071411172971.pdf) | [[code]](https://github.com/xing96/MIM-lipreading)
  
* Yuanhang Zhang, Shuang Yang, Jingyun Xiao, Shiguang Shan, Xilin Chen, Can We Read Speech Beyond the Lips? Rethinking RoI Selection for Deep Visual Speech Recognition, *FG* 2020 **(oral)**  
    [[PDF]](https://vipl.ict.ac.cn/uploadfile/upload/2020071411181845.pdf) | [[code]](https://github.com/sailordiary/deep-face-vsr)
  
* Mingshuang Luo, Shuang Yang, Shiguang Shan, Xilin Chen, “Pseudo-Convolutional Policy Gradient for Sequence-to-Sequence Lip-Reading”, *FG* 2020  
    [[PDF]](https://vipl.ict.ac.cn/uploadfile/upload/2020071411152795.pdf)
  
* Yuanhang Zhang, Jingyun Xiao, Shuang Yang, Shiguang Shan, Multi-Task Learning for Audio-Visual Active Speaker Detection, *CVPR ActivityNet Challenge* 2019  
    [[PDF]](https://static.googleusercontent.com/media/research.google.com/zh-CN//ava/2019/Multi_Task_Learning_for_Audio_Visual_Active_Speaker_Detection.pdf)

* Re-implementation of LipNet
   [[code: version-A]](https://github.com/Fengdalu/LipNet-PyTorch)(Dalu Feng)
   [[code: version-B]](https://github.com/sailordiary/LipNet-PyTorch)(Yuanhang Zhang)
